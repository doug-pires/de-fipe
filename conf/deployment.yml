# Custom section is used to store configurations that might be repetative.
# Please read YAML documentation for details on how to use substitutions and anchors.
build:
   python: "poetry"


custom:
  basic-cluster-props: &basic-cluster-props
    spark_version: "13.3.x-scala2.12"
    spark_conf:
      spark.databricks.delta.preview.enabled: 'true'
    spark_env_vars:
      MY_VAR: Douglas Pires

  basic-static-cluster: &basic-static-cluster
    new_cluster:
      <<: *basic-cluster-props
      num_workers: 1
      node_type_id: "Standard_DS3_v2"

environments:
  default:
    workflows:
      #######################################################################################
      #   Example workflow for integration tests                                            #
      #######################################################################################
      # - name: "fipe_dbx-sample-tests"
      #   tasks:
      #     - task_key: "main"
      #       <<: *basic-static-cluster
      #       spark_python_task:
      #           python_file: "file://tests/entrypoint.py"
      #           # this call supports all standard pytest arguments
      #           parameters: ["file:fuse://tests/integration", "--cov=fipe_dbx"]
      #######################################################################################
      # this is an example job with single ETL task based on 2.1 API and wheel_task format #
      ######################################################################################
      - name: "de_fipe"
        job_clusters:
          - job_cluster_key: "default"
            <<: *basic-static-cluster
            # init_scripts:
            #   - dbfs:
            #       destination: dbfs:/some/path/install_sql_driver.sh
        tasks:
          # - task_key: "install_chrome"
          #   job_cluster_key: "default"
          #   spark_python_task:
          #     python_file: "file://fipe/pipeline/install_chrome.py"
              # parameters: ["--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml"]
          - task_key: "notebook"
            deployment_config:
              no_package: true # we omit using package since code will be shipped directly from the Repo
            job_cluster_key: "default"
            notebook_task:
              notebook_path: "/Repos/dpm@dpiresmartins.onmicrosoft.com/Databricks/Install Google Chrome/Install Google Chrome Demo"

          - task_key: "provide_configuration_values"
            job_cluster_key: "default"
            spark_python_task:
              python_file: "file://fipe/scripts/provide_configuration.py"
              parameters: ["--conf-file", "file:fuse://conf/pipeline/general_config.yml"]
            depends_on:
              - task_key: "notebook"
          - task_key: "extract_on_demand"
            job_cluster_key: "default"
            python_wheel_task:
              package_name: "fipe"
              entry_point: "on_demand" # take a look at the setup.py entry_points section for details on how to define an entrypoint
              parameters: ["--conf-file", "file:fuse://conf/pipeline/general_config.yml"]
            depends_on:
              - task_key: "provide_configuration_values"
              # parameters: ["--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml"]
      #############################################################
      # this is an example multitask job with notebook task       #
      #############################################################
      # - name: "fipe_dbx-sample-multitask"
      #   job_clusters:
      #     - job_cluster_key: "default"
      #       <<: *basic-static-cluster
      #   tasks:
      #     - task_key: "etl"
      #       job_cluster_key: "default"
      #       spark_python_task:
      #         python_file: "file://fipe_dbx/tasks/sample_etl_task.py"
      #         parameters: [ "--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml" ]
      #     - task_key: "ml"
      #       depends_on:
      #         - task_key: "etl"
      #       job_cluster_key: "default"
      #       python_wheel_task:
      #         package_name: "fipe_dbx"
      #         entry_point: "ml"
      #         parameters: [ "--conf-file", "file:fuse://conf/tasks/sample_ml_config.yml" ]
          ###############################################################################
          # this is an example task based on the notebook                               #
          # Please note that first you'll need to add a Repo and commit notebook to it. #
          ###############################################################################
          # - task_key: "notebook"
          #   deployment_config:
          #     no_package: true # we omit using package since code will be shipped directly from the Repo
          #   depends_on:
          #     - task_key: "ml"
          #   job_cluster_key: "default"
          #   notebook_task:
          #     notebook_path: "/Repos/Staging/fipe_dbx/notebooks/sample_notebook"

